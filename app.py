"""
Daegu Public Data Visualization - Streamlit Application

An educational tool for exploring and analyzing public datasets from Daegu.
Provides individual dataset exploration, cross-dataset spatial analysis, and
educational content to help data analysis learners discover insights independently.
"""
import io
import time
import streamlit as st
import pandas as pd
import plotly.express as px
from streamlit_folium import st_folium
from utils.loader import load_dataset, load_dataset_from_session, get_dataset_info, read_csv_safe, read_uploaded_csv
from utils.geo import detect_lat_lng_columns
from utils.visualizer import (
    plot_numeric_distribution,
    plot_categorical_distribution,
    plot_boxplot,
    plot_kde,
    plot_scatter,
    plot_with_options,
    check_missing_ratio,
    create_folium_map,
    create_overlay_map
)
from utils.geo import compute_proximity_stats
from utils.narration import (
    summarize_proximity_stats,
    generate_distribution_insight,
    compare_distributions
)
from utils.chatbot import (
    SYSTEM_PROMPT,
    create_data_context,
    create_chat_response,
    create_chat_response_with_tools,
    stream_chat_response_with_tools,
    handle_chat_error,
    validate_api_key
)
from anthropic import Anthropic

# ë°ì´í„°ì…‹ ë§¤í•‘ ìƒìˆ˜
DATASET_MAPPING = {
    'cctv': {
        'display_name': 'CCTV',
        'tab_icon': 'ğŸ¥',
        'expected_file': 'ëŒ€êµ¬ CCTV ì •ë³´.csv',
        'color': 'red'
    },
    'lights': {
        'display_name': 'ë³´ì•ˆë“±',
        'tab_icon': 'ğŸ’¡',
        'expected_file': 'ëŒ€êµ¬ ë³´ì•ˆë“± ì •ë³´.csv',
        'color': 'blue'
    },
    'zones': {
        'display_name': 'ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­',
        'tab_icon': 'ğŸ«',
        'expected_file': 'ëŒ€êµ¬ ì–´ë¦°ì´ ë³´í˜¸ êµ¬ì—­ ì •ë³´.csv',
        'color': 'green'
    },
    'parking': {
        'display_name': 'ì£¼ì°¨ì¥',
        'tab_icon': 'ğŸ…¿ï¸',
        'expected_file': 'ëŒ€êµ¬ ì£¼ì°¨ì¥ ì •ë³´.csv',
        'color': 'purple'
    },
    'accident': {
        'display_name': 'ì‚¬ê³ ',
        'tab_icon': 'ğŸš—',
        'expected_file': 'countrywide_accident.csv',
        'color': 'orange'
    },
    'train': {
        'display_name': 'í›ˆë ¨ ë°ì´í„°',
        'tab_icon': 'ğŸ“Š',
        'expected_file': 'train.csv',
        'color': 'darkred'
    },
    'test': {
        'display_name': 'í…ŒìŠ¤íŠ¸ ë°ì´í„°',
        'tab_icon': 'ğŸ“‹',
        'expected_file': 'test.csv',
        'color': 'darkblue'
    }
}

# AI ëª¨ë¸ ì˜µì…˜
AI_MODEL_OPTIONS = [
    {'id': 'claude-sonnet-4-5-20250929', 'name': 'Claude Sonnet 4.5', 'description': 'ë¹ ë¥¸ ì‘ë‹µ, ë¹„ìš© íš¨ìœ¨ì  (ê¶Œì¥)'},
    {'id': 'claude-opus-4-5-20251101', 'name': 'Claude Opus 4.5', 'description': 'ë³µì¡í•œ ë¶„ì„ì— ì í•©'},
    {'id': 'claude-haiku-4-5-20251001', 'name': 'Claude Haiku 4.5', 'description': 'ê°„ë‹¨í•œ ì§ˆë¬¸ì— ìµœì '}
]


def init_session_state():
    """
    session_state ì´ˆê¸°í™”.
    ì•± ì‹œì‘ ì‹œ í•œ ë²ˆ í˜¸ì¶œ.
    """
    if 'initialized' in st.session_state:
        return

    # ë°ì´í„°ì…‹ ì €ì¥ì†Œ
    if 'datasets' not in st.session_state:
        st.session_state.datasets = {}

    # ì—…ë¡œë“œ ìƒíƒœ
    if 'upload_status' not in st.session_state:
        st.session_state.upload_status = {
            key: False for key in DATASET_MAPPING.keys()
        }

    # ì±—ë´‡ ì„¸ì…˜
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = {
            'api_key': '',
            'model': 'claude-sonnet-4-5-20250929',
            'selected_dataset': None,
            'chat_history': {},  # T035: Dataset-specific chat history
            'tokens': {'total': 0, 'input': 0, 'output': 0}
        }

    # ì§€ë„ ì„¤ì •
    if 'map_settings' not in st.session_state:
        st.session_state.map_settings = {
            'max_points': 5000,  # ê¸°ë³¸ê°’
            'confirmed': False   # Enter í‚¤ ì…ë ¥ ì—¬ë¶€
        }

    st.session_state.initialized = True


def get_chat_history(dataset_name: str) -> list:
    """
    Get chat history for a specific dataset. (T036)

    Parameters:
        dataset_name (str): Dataset key (e.g., 'cctv', 'lights')

    Returns:
        list: Chat history for the dataset
    """
    if dataset_name not in st.session_state.chatbot['chat_history']:
        st.session_state.chatbot['chat_history'][dataset_name] = []
    return st.session_state.chatbot['chat_history'][dataset_name]


def clear_chat_history(dataset_name: str) -> None:
    """
    Clear chat history for a specific dataset. (T037)

    Parameters:
        dataset_name (str): Dataset key (e.g., 'cctv', 'lights')
    """
    st.session_state.chatbot['chat_history'][dataset_name] = []


# Page configuration
st.set_page_config(
    page_title="ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™”",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)


def render_dataset_tab(dataset_name: str, dataset_display_name: str):
    """
    Render a complete tab for exploring an individual dataset.

    Parameters:
        dataset_name (str): Internal dataset name for load_dataset()
        dataset_display_name (str): Display name for UI
    """
    st.header(f"{dataset_display_name} ë°ì´í„°ì…‹")

    # Check if dataset is uploaded (T020, T021)
    if not st.session_state.upload_status.get(dataset_name, False):
        st.info(f"ğŸ“¤ **{dataset_display_name}** ë°ì´í„°ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.markdown("**í”„ë¡œì íŠ¸ ê°œìš”** íƒ­ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    # Load dataset from session_state (T022)
    df = load_dataset_from_session(dataset_name)
    if df is None:
        st.warning(f"âš ï¸ {dataset_display_name} ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # Get dataset info
    info = get_dataset_info(df)

    # Display basic statistics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì „ì²´ í–‰ ìˆ˜", f"{info['row_count']:,}")
    with col2:
        st.metric("ì „ì²´ ì»¬ëŸ¼ ìˆ˜", info['column_count'])
    with col3:
        missing_pct = sum(info['missing_ratios'].values()) / len(info['missing_ratios']) * 100 if info['missing_ratios'] else 0
        st.metric("í‰ê·  ê²°ì¸¡ê°’ %", f"{missing_pct:.1f}%")

    # Data Preview
    with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 10ê°œ í–‰)", expanded=False):
        st.dataframe(df.head(10), use_container_width=True)

    # Column Information
    with st.expander("ğŸ“Š ì»¬ëŸ¼ ì •ë³´", expanded=False):
        col_info_df = []
        for col in df.columns:
            col_info_df.append({
                'ì»¬ëŸ¼': col,
                'íƒ€ì…': info['dtypes'][col],
                'ê²°ì¸¡ê°’ %': f"{info['missing_ratios'][col] * 100:.1f}%"
            })
        st.dataframe(col_info_df, use_container_width=True)

    # Descriptive Statistics for Numeric Columns
    if not info['numeric_summary'].empty:
        with st.expander("ğŸ“ˆ ìˆ«ì ì»¬ëŸ¼ í†µê³„", expanded=False):
            st.dataframe(info['numeric_summary'], use_container_width=True)

    # Visualizations
    st.subheader("ì‹œê°í™”")

    # Detect coordinates
    lat_col, lng_col = detect_lat_lng_columns(df)

    # Map Visualization
    if lat_col and lng_col:
        st.markdown("### ğŸ—ºï¸ ì§€ë¦¬ì  ë¶„í¬")
        st.info(f"ê°ì§€ëœ ì¢Œí‘œ: **{lat_col}** (ìœ„ë„), **{lng_col}** (ê²½ë„)")

        # ì§€ë„ ì„¤ì • í™•ì¸ ì—¬ë¶€ ì²´í¬
        if not st.session_state.map_settings['confirmed']:
            st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ ì§€ë„ ì„¤ì •ì„ í™•ì¸í•˜ê³  Enter í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        else:
            # Get columns for popup (exclude coordinate columns, limit to first 3 non-numeric)
            popup_candidates = [col for col in df.columns if col not in [lat_col, lng_col]]
            popup_cols = popup_candidates[:3]  # Show first 3 columns in popup

            # ì§€ë„ ìºì‹œ í‚¤ì— max_points í¬í•¨
            max_points = st.session_state.map_settings['max_points']
            cache_key = f"map_{dataset_name}_{len(df)}_{max_points}"

            if cache_key not in st.session_state:
                st.session_state[cache_key] = create_folium_map(
                    df, lat_col, lng_col,
                    popup_cols=popup_cols,
                    color='blue',
                    name=dataset_display_name,
                    max_points=max_points
                )

            # T042: Display map with returned_objects=[] to prevent rerendering
            st_folium(st.session_state[cache_key], width=700, height=500, returned_objects=[])
    else:
        st.info("â„¹ï¸ ì§€ë¦¬ ì¢Œí‘œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì—ëŠ” ì§€ë„ ì‹œê°í™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Numeric Distributions (T029-T033: ì°¨íŠ¸ ìœ í˜• ì„ íƒ, ê²°ì¸¡ì¹˜ ê²½ê³ )
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    if numeric_cols:
        st.markdown("### ğŸ“Š ìˆ«ì ì»¬ëŸ¼ ë¶„í¬")

        # T029: Chart type selection
        chart_type = st.selectbox(
            "ì°¨íŠ¸ ìœ í˜• ì„ íƒ:",
            options=['íˆìŠ¤í† ê·¸ë¨', 'ë°•ìŠ¤í”Œë¡¯', 'KDE', 'ì‚°ì ë„'],
            key=f"{dataset_name}_chart_type"
        )

        chart_type_map = {
            'íˆìŠ¤í† ê·¸ë¨': 'histogram',
            'ë°•ìŠ¤í”Œë¡¯': 'boxplot',
            'KDE': 'kde',
            'ì‚°ì ë„': 'scatter'
        }

        # T030: For scatter plot, show X/Y column selection
        if chart_type == 'ì‚°ì ë„':
            col1, col2 = st.columns(2)
            with col1:
                x_col = st.selectbox(
                    "Xì¶• ì»¬ëŸ¼:",
                    options=numeric_cols,
                    key=f"{dataset_name}_scatter_x"
                )
            with col2:
                y_col = st.selectbox(
                    "Yì¶• ì»¬ëŸ¼:",
                    options=[c for c in numeric_cols if c != x_col] if len(numeric_cols) > 1 else numeric_cols,
                    key=f"{dataset_name}_scatter_y"
                )

            # T033: Missing value warning for scatter
            x_warning, x_ratio = check_missing_ratio(df, x_col)
            y_warning, y_ratio = check_missing_ratio(df, y_col)
            if x_warning:
                st.warning(f"âš ï¸ {x_col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ {x_ratio*100:.1f}%ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            if y_warning:
                st.warning(f"âš ï¸ {y_col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ {y_ratio*100:.1f}%ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # T031: Render scatter plot
            fig = plot_scatter(df, x_col, y_col)
            st.plotly_chart(fig, use_container_width=True)
        else:
            # Single column selection for other chart types
            selected_numeric_col = st.selectbox(
                "ì‹œê°í™”í•  ìˆ«ì ì»¬ëŸ¼ ì„ íƒ:",
                options=numeric_cols,
                key=f"{dataset_name}_numeric_select"
            )

            if selected_numeric_col:
                # T033: Missing value warning
                is_high_missing, missing_ratio = check_missing_ratio(df, selected_numeric_col)
                if is_high_missing:
                    st.warning(f"âš ï¸ {selected_numeric_col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ {missing_ratio*100:.1f}%ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                # T031: Render selected chart type
                fig = plot_with_options(df, selected_numeric_col, chart_type_map[chart_type])
                st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("â„¹ï¸ ì´ ë°ì´í„°ì…‹ì—ëŠ” ìˆ«ìí˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # Categorical Distributions
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    if categorical_cols:
        st.markdown("### ğŸ“Š ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„í¬")

        # Let user select which categorical column to visualize
        selected_cat_col = st.selectbox(
            "ì‹œê°í™”í•  ë²”ì£¼í˜• ì»¬ëŸ¼ ì„ íƒ:",
            options=categorical_cols,
            key=f"{dataset_name}_cat_select"
        )

        if selected_cat_col:
            fig = plot_categorical_distribution(df, selected_cat_col)
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("â„¹ï¸ ì´ ë°ì´í„°ì…‹ì—ëŠ” ë²”ì£¼í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_overview_tab():
    """
    Render the project overview tab with upload functionality. (T016-T019)
    """
    st.header("ğŸ“– í”„ë¡œì íŠ¸ ê°œìš”")

    # Project Introduction
    st.markdown("""
    ## ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™” í”„ë¡œì íŠ¸

    ì´ í”„ë¡œì íŠ¸ëŠ” ëŒ€êµ¬ì‹œì˜ ë‹¤ì–‘í•œ ê³µê³µ ë°ì´í„°ë¥¼ íƒìƒ‰í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
    ë°ì´í„° ë¶„ì„ì„ í•™ìŠµí•˜ëŠ” ì‚¬ìš©ìë“¤ì´ ì‹¤ì œ ê³µê³µ ë°ì´í„°ë¥¼ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°œê²¬í•˜ê³ 
    ë°ì´í„° ì‹œê°í™” ê¸°ìˆ ì„ ìµí ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
    """)

    # Data Upload Section (T017-T019)
    st.subheader("ğŸ“¤ ë°ì´í„° ì—…ë¡œë“œ")
    st.markdown("ê° ë°ì´í„°ì…‹ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    # Display upload status
    uploaded_count = sum(st.session_state.upload_status.values())
    st.info(f"ì—…ë¡œë“œ í˜„í™©: {uploaded_count} / {len(DATASET_MAPPING)} ë°ì´í„°ì…‹")

    # Create upload widgets for each dataset
    for dataset_key, dataset_info in DATASET_MAPPING.items():
        with st.expander(
            f"{dataset_info['tab_icon']} {dataset_info['display_name']} "
            f"({'âœ… ì—…ë¡œë“œë¨' if st.session_state.upload_status.get(dataset_key) else 'â³ ëŒ€ê¸°ì¤‘'})"
        ):
            st.markdown(f"**ì˜ˆìƒ íŒŒì¼ëª…**: `{dataset_info['expected_file']}`")

            uploaded_file = st.file_uploader(
                f"{dataset_info['display_name']} CSV íŒŒì¼ ì„ íƒ",
                type=['csv'],
                key=f"upload_{dataset_key}"
            )

            if uploaded_file is not None:
                try:
                    df = read_uploaded_csv(uploaded_file)
                    # Store in session_state (T018)
                    st.session_state.datasets[dataset_key] = df
                    st.session_state.upload_status[dataset_key] = True

                    # Display upload info (T019)
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("íŒŒì¼ëª…", uploaded_file.name)
                    with col2:
                        file_size_kb = uploaded_file.size / 1024
                        if file_size_kb > 1024:
                            st.metric("íŒŒì¼ í¬ê¸°", f"{file_size_kb/1024:.2f} MB")
                        else:
                            st.metric("íŒŒì¼ í¬ê¸°", f"{file_size_kb:.2f} KB")
                    with col3:
                        st.metric("í–‰ x ì»¬ëŸ¼", f"{len(df):,} x {len(df.columns)}")

                    st.success(f"âœ… {dataset_info['display_name']} ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ!")

                    # Show preview
                    with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        st.dataframe(df.head(5), use_container_width=True)

                except Exception as e:
                    st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")

    st.markdown("---")

    # Technical Information (T045) - v1.1.3: ì£¼ìš” ê¸°ëŠ¥, ì‚¬ìš© ë°©ë²• ì„¹ì…˜ ì‚­ì œ
    st.subheader("ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ")

    tech_col1, tech_col2, tech_col3 = st.columns(3)

    with tech_col1:
        st.markdown("""
        **í”„ë¡ íŠ¸ì—”ë“œ**
        - Streamlit 1.28+
        - Plotly 5.17+
        - Folium 0.14+
        """)

    with tech_col2:
        st.markdown("""
        **ë°ì´í„° ì²˜ë¦¬**
        - Pandas 2.0+
        - NumPy 1.24+
        """)

    with tech_col3:
        st.markdown("""
        **AI**
        - Anthropic Claude
        - Python 3.10+
        """)

    # v1.1.3: ì‹œìŠ¤í…œ êµ¬ì¡°, ë°ì´í„° ë¶„ì„ ê¸°ì´ˆ ê°œë…, ë¶„ì„ ê°€ì´ë“œ ì§ˆë¬¸, êµì°¨ ë°ì´í„° ë¶„ì„ ì¤‘ìš”ì„± ì„¹ì…˜ ì‚­ì œ


def render_cross_analysis_tab():
    """
    Render the cross-data analysis tab with proximity analysis, overlay maps,
    distribution comparison, and natural language insights. (T028-T041)
    """
    st.header("ğŸ”„ êµì°¨ ë°ì´í„° ë¶„ì„")
    st.markdown("""
    ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ë™ì‹œì— ì§€ë„ ìœ„ì— í‘œì‹œí•˜ê³ , ê³µê°„ì  ê·¼ì ‘ì„±ì„ ë¶„ì„í•˜ë©°,
    ë¶„í¬ë¥¼ ë¹„êµí•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°œê²¬í•©ë‹ˆë‹¤.
    """)

    # Check if any datasets are uploaded
    uploaded_datasets = [
        key for key, uploaded in st.session_state.upload_status.items()
        if uploaded
    ]

    if not uploaded_datasets:
        st.info("ğŸ“¤ ë¨¼ì € **í”„ë¡œì íŠ¸ ê°œìš”** íƒ­ì—ì„œ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # Dataset selection
    st.subheader("ğŸ“Š ë°ì´í„°ì…‹ ì„ íƒ")

    available_options = {
        DATASET_MAPPING[key]['display_name']: key
        for key in uploaded_datasets
    }

    # Multi-select for datasets
    selected_names = st.multiselect(
        "ë¶„ì„í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš” (2ê°œ ì´ìƒ ê¶Œì¥):",
        options=list(available_options.keys()),
        default=list(available_options.keys())[:2] if len(available_options) >= 2 else list(available_options.keys())
    )

    if len(selected_names) == 0:
        st.warning("âš ï¸ ìµœì†Œ 1ê°œ ì´ìƒì˜ ë°ì´í„°ì…‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    # Load selected datasets
    datasets_to_overlay = []
    datasets_with_coords = {}
    dataset_colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'darkblue']

    for idx, name in enumerate(selected_names):
        dataset_key = available_options[name]
        df = load_dataset_from_session(dataset_key)

        if df is not None:
            lat_col, lng_col = detect_lat_lng_columns(df)

            if lat_col and lng_col:
                popup_candidates = [col for col in df.columns if col not in [lat_col, lng_col]]
                popup_cols = popup_candidates[:3]

                datasets_to_overlay.append({
                    'df': df,
                    'lat_col': lat_col,
                    'lng_col': lng_col,
                    'popup_cols': popup_cols,
                    'color': dataset_colors[idx % len(dataset_colors)],
                    'name': name,
                    'icon': 'info-sign'
                })
                datasets_with_coords[name] = {
                    'df': df,
                    'lat_col': lat_col,
                    'lng_col': lng_col,
                    'key': dataset_key
                }
            else:
                st.warning(f"âš ï¸ {name} ë°ì´í„°ì…‹ì—ì„œ ì¢Œí‘œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì§€ë„ ë° ê·¼ì ‘ ë¶„ì„ ì œì™¸)")

    # Create tabs for different analysis types
    analysis_tabs = st.tabs(["ğŸ—ºï¸ í†µí•© ì§€ë„", "ğŸ“ ê·¼ì ‘ ë¶„ì„", "ğŸ“ˆ ë¶„í¬ ë¹„êµ"])

    # Tab 1: Overlay Map (T035)
    with analysis_tabs[0]:
        if datasets_to_overlay:
            st.subheader("ğŸ—ºï¸ í†µí•© ì§€ë„ ì‹œê°í™”")

            # Show legend
            st.markdown("**ë²”ë¡€:**")
            legend_cols = st.columns(min(len(datasets_to_overlay), 4))
            for idx, ds in enumerate(datasets_to_overlay):
                with legend_cols[idx % 4]:
                    color_emoji = {'red': 'ğŸ”´', 'blue': 'ğŸ”µ', 'green': 'ğŸŸ¢', 'purple': 'ğŸŸ£', 'orange': 'ğŸŸ ', 'darkred': 'ğŸ”´', 'darkblue': 'ğŸ”µ'}
                    emoji = color_emoji.get(ds['color'], 'âšª')
                    st.markdown(f"{emoji} **{ds['name']}** ({len(ds['df']):,}ê°œ)")

            # Overlay map caching with session_state
            overlay_cache_key = f"overlay_map_{len(datasets_to_overlay)}_{sum(len(ds['df']) for ds in datasets_to_overlay)}"
            if overlay_cache_key not in st.session_state:
                st.session_state[overlay_cache_key] = create_overlay_map(datasets_to_overlay)

            # Display map with returned_objects=[] to prevent rerendering
            st_folium(st.session_state[overlay_cache_key], width=900, height=600, returned_objects=[])

            st.info("ğŸ’¡ ì§€ë„ ìš°ì¸¡ ìƒë‹¨ì˜ ë ˆì´ì–´ ì»¨íŠ¸ë¡¤ì„ ì‚¬ìš©í•˜ì—¬ ê° ë°ì´í„°ì…‹ì„ ê°œë³„ì ìœ¼ë¡œ ì¼œê³  ëŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("âš ï¸ ì¢Œí‘œ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")

    # Tab 2: Proximity Analysis (T034, T037-T041)
    with analysis_tabs[1]:
        st.subheader("ğŸ“ ê·¼ì ‘ ë¶„ì„")
        st.markdown("""
        ë‘ ë°ì´í„°ì…‹ ê°„ì˜ ê³µê°„ì  ê·¼ì ‘ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
        ê¸°ì¤€ ë°ì´í„°ì…‹ì˜ ê° í¬ì¸íŠ¸ì—ì„œ ëŒ€ìƒ ë°ì´í„°ì…‹ì˜ í¬ì¸íŠ¸ê°€ íŠ¹ì • ê±°ë¦¬ ë‚´ì— ëª‡ ê°œ ìˆëŠ”ì§€ ê³„ì‚°í•©ë‹ˆë‹¤.
        """)

        if len(datasets_with_coords) < 2:
            st.warning("âš ï¸ ê·¼ì ‘ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì¢Œí‘œ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„°ì…‹ì´ ìµœì†Œ 2ê°œ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            coord_dataset_names = list(datasets_with_coords.keys())

            col1, col2 = st.columns(2)
            with col1:
                base_name = st.selectbox(
                    "ê¸°ì¤€ ë°ì´í„°ì…‹:",
                    options=coord_dataset_names,
                    key="proximity_base"
                )
            with col2:
                target_options = [n for n in coord_dataset_names if n != base_name]
                target_name = st.selectbox(
                    "ëŒ€ìƒ ë°ì´í„°ì…‹:",
                    options=target_options,
                    key="proximity_target"
                )

            # Threshold selection
            st.markdown("**ë¶„ì„ ê±°ë¦¬ ì„ê³„ê°’ (km):**")
            threshold_col1, threshold_col2, threshold_col3 = st.columns(3)
            with threshold_col1:
                t1 = st.number_input("ì„ê³„ê°’ 1", value=0.5, min_value=0.1, max_value=10.0, step=0.1, key="t1")
            with threshold_col2:
                t2 = st.number_input("ì„ê³„ê°’ 2", value=1.0, min_value=0.1, max_value=10.0, step=0.1, key="t2")
            with threshold_col3:
                t3 = st.number_input("ì„ê³„ê°’ 3", value=2.0, min_value=0.1, max_value=10.0, step=0.1, key="t3")

            thresholds = sorted([t1, t2, t3])

            if st.button("ğŸ” ê·¼ì ‘ ë¶„ì„ ì‹¤í–‰", key="run_proximity"):
                base_data = datasets_with_coords[base_name]
                target_data = datasets_with_coords[target_name]

                # Show progress
                with st.spinner(f"'{base_name}'ê³¼(ì™€) '{target_name}' ê°„ì˜ ê·¼ì ‘ ë¶„ì„ ì¤‘..."):
                    try:
                        # Run proximity analysis (T034)
                        proximity_df = compute_proximity_stats(
                            base_data['df'],
                            base_data['lat_col'],
                            base_data['lng_col'],
                            target_data['df'],
                            target_data['lat_col'],
                            target_data['lng_col'],
                            thresholds=thresholds
                        )

                        if proximity_df.empty:
                            st.error("âŒ ê·¼ì ‘ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì¢Œí‘œ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        else:
                            # Display results table
                            st.markdown("### ğŸ“Š ê·¼ì ‘ ë¶„ì„ ê²°ê³¼")

                            # Summary statistics
                            summary_data = []
                            for t in thresholds:
                                t_str = str(t)
                                if t_str in proximity_df.columns:
                                    summary_data.append({
                                        'ê±°ë¦¬ ì„ê³„ê°’': f"{t}km",
                                        'í‰ê· ': f"{proximity_df[t_str].mean():.2f}",
                                        'ì¤‘ì•™ê°’': f"{proximity_df[t_str].median():.1f}",
                                        'ìµœì†Œ': f"{proximity_df[t_str].min():.0f}",
                                        'ìµœëŒ€': f"{proximity_df[t_str].max():.0f}",
                                        'í‘œì¤€í¸ì°¨': f"{proximity_df[t_str].std():.2f}"
                                    })

                            st.dataframe(summary_data, use_container_width=True)

                            # Natural language insights (T037)
                            st.markdown("### ğŸ’¡ ë¶„ì„ ì¸ì‚¬ì´íŠ¸")
                            for t in thresholds:
                                t_str = str(t)
                                if t_str in proximity_df.columns:
                                    insight = summarize_proximity_stats(proximity_df, t_str, target_name)
                                    st.markdown(f"**{t}km ë°˜ê²½:** {insight}")

                            # Store results in session state for potential reuse
                            st.session_state['last_proximity_result'] = proximity_df

                    except Exception as e:
                        st.error(f"âŒ ê·¼ì ‘ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    # Tab 3: Distribution Comparison (T036, T038, T039)
    with analysis_tabs[2]:
        st.subheader("ğŸ“ˆ ë¶„í¬ ë¹„êµ")
        st.markdown("""
        ë‘ ë°ì´í„°ì…‹ì˜ ìˆ«ìí˜• ì»¬ëŸ¼ ë¶„í¬ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
        ë™ì¼í•œ ì»¬ëŸ¼ëª…ì„ ê°€ì§„ ê²½ìš° ì§ì ‘ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        """)

        if len(selected_names) < 2:
            st.warning("âš ï¸ ë¶„í¬ ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ë°ì´í„°ì…‹ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            # Select datasets to compare
            col1, col2 = st.columns(2)
            with col1:
                compare_name1 = st.selectbox(
                    "ì²« ë²ˆì§¸ ë°ì´í„°ì…‹:",
                    options=selected_names,
                    key="compare_ds1"
                )
            with col2:
                compare_options2 = [n for n in selected_names if n != compare_name1]
                compare_name2 = st.selectbox(
                    "ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹:",
                    options=compare_options2,
                    key="compare_ds2"
                )

            # Load datasets
            df1 = load_dataset_from_session(available_options[compare_name1])
            df2 = load_dataset_from_session(available_options[compare_name2])

            if df1 is not None and df2 is not None:
                # Find common numeric columns
                numeric_cols1 = set(df1.select_dtypes(include=['number']).columns)
                numeric_cols2 = set(df2.select_dtypes(include=['number']).columns)
                common_numeric = list(numeric_cols1.intersection(numeric_cols2))

                # Column selection
                st.markdown("**ë¹„êµí•  ì»¬ëŸ¼ ì„ íƒ:**")

                col1, col2 = st.columns(2)
                with col1:
                    all_numeric1 = list(df1.select_dtypes(include=['number']).columns)
                    selected_col1 = st.selectbox(
                        f"{compare_name1} ì»¬ëŸ¼:",
                        options=all_numeric1 if all_numeric1 else ["(ìˆ«ìí˜• ì»¬ëŸ¼ ì—†ìŒ)"],
                        key="compare_col1"
                    )
                with col2:
                    all_numeric2 = list(df2.select_dtypes(include=['number']).columns)
                    # Default to same column if common
                    default_idx = 0
                    if selected_col1 in all_numeric2:
                        default_idx = all_numeric2.index(selected_col1)
                    selected_col2 = st.selectbox(
                        f"{compare_name2} ì»¬ëŸ¼:",
                        options=all_numeric2 if all_numeric2 else ["(ìˆ«ìí˜• ì»¬ëŸ¼ ì—†ìŒ)"],
                        index=default_idx if all_numeric2 else 0,
                        key="compare_col2"
                    )

                if selected_col1 != "(ìˆ«ìí˜• ì»¬ëŸ¼ ì—†ìŒ)" and selected_col2 != "(ìˆ«ìí˜• ì»¬ëŸ¼ ì—†ìŒ)":
                    # Display comparison chart
                    st.markdown("### ğŸ“Š ë¶„í¬ ë¹„êµ ì°¨íŠ¸")

                    # Create overlayed histogram
                    fig = px.histogram(
                        pd.DataFrame({
                            f'{compare_name1} - {selected_col1}': df1[selected_col1].dropna(),
                        }),
                        title=f"ë¶„í¬ ë¹„êµ: {selected_col1} vs {selected_col2}",
                        opacity=0.7,
                        barmode='overlay'
                    )

                    # Add second histogram
                    fig.add_trace(
                        px.histogram(
                            pd.DataFrame({
                                f'{compare_name2} - {selected_col2}': df2[selected_col2].dropna(),
                            }),
                            opacity=0.7
                        ).data[0]
                    )

                    fig.update_layout(
                        xaxis_title="ê°’",
                        yaxis_title="ë¹ˆë„",
                        legend_title="ë°ì´í„°ì…‹",
                        barmode='overlay'
                    )

                    st.plotly_chart(fig, use_container_width=True)

                    # Distribution comparison insight
                    st.markdown("### ğŸ’¡ ë¹„êµ ì¸ì‚¬ì´íŠ¸")
                    comparison_insight = compare_distributions(df1, selected_col1, df2, selected_col2)
                    st.markdown(comparison_insight)

                    # Individual distribution insights
                    with st.expander("ğŸ“ ê°œë³„ ë¶„í¬ ë¶„ì„", expanded=False):
                        st.markdown(f"**{compare_name1} - {selected_col1}:**")
                        st.markdown(generate_distribution_insight(df1, selected_col1))
                        st.markdown(f"\n**{compare_name2} - {selected_col2}:**")
                        st.markdown(generate_distribution_insight(df2, selected_col2))
                else:
                    st.info("â„¹ï¸ ë¹„êµí•  ìˆ«ìí˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_sidebar():
    """
    Render the sidebar with API key input and status. (T041-T044)
    """
    with st.sidebar:
        st.header("ğŸ¤– AI ì„¤ì •")

        # T041: API Key input
        api_key = st.text_input(
            "Anthropic API Key",
            type="password",
            placeholder="sk-ant-...",
            help="Anthropic API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”. https://console.anthropic.com ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            key="sidebar_api_key"
        )

        if api_key:
            if validate_api_key(api_key):
                st.session_state.chatbot['api_key'] = api_key
                st.success("âœ… API Key ì„¤ì •ë¨")
            else:
                st.error("âŒ API Key í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
        else:
            st.info("API Keyë¥¼ ì…ë ¥í•˜ë©´ AI ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

        # ì§€ë„ ì„¤ì •
        st.subheader("ğŸ—ºï¸ ì§€ë„ ì„¤ì •")

        with st.form(key="map_settings_form", clear_on_submit=False):
            map_points_input = st.text_input(
                "ìµœëŒ€ í‘œì‹œ í¬ì¸íŠ¸ ìˆ˜",
                value=str(st.session_state.map_settings['max_points']),
                placeholder="5000",
                help="ì§€ë„ì— í‘œì‹œí•  ìµœëŒ€ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì…ë‹ˆë‹¤."
            )
            st.caption("ê¸°ë³¸ê°’: 5000")

            # ìˆ¨ê¹€ submit ë²„íŠ¼ (Enter í‚¤ë¡œ ì œì¶œ)
            submitted = st.form_submit_button("ì ìš©", use_container_width=True)

            if submitted:
                try:
                    new_val = int(map_points_input) if map_points_input else 5000
                    if new_val <= 0:
                        st.error("âŒ 1 ì´ìƒì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
                    else:
                        st.session_state.map_settings['max_points'] = new_val
                        st.session_state.map_settings['confirmed'] = True
                        # ì§€ë„ ìºì‹œ ì´ˆê¸°í™”
                        keys_to_delete = [k for k in list(st.session_state.keys()) if k.startswith('map_') and k != 'map_settings']
                        for k in keys_to_delete:
                            del st.session_state[k]
                except ValueError:
                    st.error("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")

        # ìƒíƒœ í‘œì‹œ (form ì™¸ë¶€)
        if st.session_state.map_settings['confirmed']:
            st.success(f"âœ… ì§€ë„ í¬ì¸íŠ¸ ìˆ˜: {st.session_state.map_settings['max_points']:,}ê°œ")
        else:
            st.info("Enter í‚¤ ë˜ëŠ” ì ìš© ë²„íŠ¼ì„ ëˆŒëŸ¬ ì§€ë„ ì„¤ì •ì„ ì ìš©í•˜ì„¸ìš”")

        # T042: Model selection
        st.subheader("ëª¨ë¸ ì„ íƒ")
        model_options = {opt['name']: opt['id'] for opt in AI_MODEL_OPTIONS}
        model_descriptions = {opt['name']: opt['description'] for opt in AI_MODEL_OPTIONS}

        selected_model_name = st.selectbox(
            "AI ëª¨ë¸",
            options=list(model_options.keys()),
            help=model_descriptions.get(list(model_options.keys())[0], ""),
            key="sidebar_model"
        )
        st.session_state.chatbot['model'] = model_options[selected_model_name]
        st.caption(model_descriptions[selected_model_name])

        # T043: Token usage display
        st.subheader("ğŸ“Š í† í° ì‚¬ìš©ëŸ‰")
        tokens = st.session_state.chatbot['tokens']
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì…ë ¥", f"{tokens['input']:,}")
        with col2:
            st.metric("ì¶œë ¥", f"{tokens['output']:,}")
        st.metric("ì´ê³„", f"{tokens['total']:,}")

        st.markdown("---")

        # T044: Upload status display
        st.subheader("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ í˜„í™©")
        uploaded_count = sum(st.session_state.upload_status.values())
        st.progress(uploaded_count / len(DATASET_MAPPING))
        st.caption(f"{uploaded_count} / {len(DATASET_MAPPING)} ë°ì´í„°ì…‹ ì—…ë¡œë“œë¨")

        for key, status in st.session_state.upload_status.items():
            icon = "âœ…" if status else "â³"
            st.text(f"{icon} {DATASET_MAPPING[key]['display_name']}")


def render_chatbot_tab():
    """
    Render the chatbot tab for data Q&A. (T045-T050)
    """
    st.header("ğŸ’¬ ë°ì´í„° ì§ˆì˜ì‘ë‹µ")
    st.markdown("ì—…ë¡œë“œí•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ AIì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”.")

    # T050: Check API Key
    api_key = st.session_state.chatbot.get('api_key', '')
    if not api_key or not validate_api_key(api_key):
        st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ Anthropic API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.info("""
        **API Key ë°œê¸‰ ë°©ë²•:**
        1. [Anthropic Console](https://console.anthropic.com) ë°©ë¬¸
        2. ê³„ì • ìƒì„± ë˜ëŠ” ë¡œê·¸ì¸
        3. API Keys ë©”ë‰´ì—ì„œ ìƒˆ í‚¤ ìƒì„±
        4. ìƒì„±ëœ í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥
        """)
        return

    # Check uploaded datasets
    uploaded_datasets = {
        DATASET_MAPPING[key]['display_name']: key
        for key, uploaded in st.session_state.upload_status.items()
        if uploaded
    }

    if not uploaded_datasets:
        st.info("ğŸ“¤ ë¨¼ì € **í”„ë¡œì íŠ¸ ê°œìš”** íƒ­ì—ì„œ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # T046: Dataset selection
    selected_display_name = st.selectbox(
        "ë¶„ì„í•  ë°ì´í„°ì…‹ ì„ íƒ:",
        options=list(uploaded_datasets.keys()),
        key="chatbot_dataset"
    )
    selected_dataset_key = uploaded_datasets[selected_display_name]
    st.session_state.chatbot['selected_dataset'] = selected_dataset_key

    # Load selected dataset
    df = load_dataset_from_session(selected_dataset_key)
    if df is None:
        st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # Show dataset summary
    with st.expander("ğŸ“Š ë°ì´í„°ì…‹ ìš”ì•½", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í–‰ ìˆ˜", f"{len(df):,}")
        with col2:
            st.metric("ì»¬ëŸ¼ ìˆ˜", len(df.columns))
        with col3:
            total_cells = len(df) * len(df.columns)
            missing_pct = (df.isnull().sum().sum() / total_cells * 100) if total_cells > 0 else 0
            st.metric("ì „ì²´ ê²°ì¸¡ë¥ ", f"{missing_pct:.1f}%")
        st.dataframe(df.head(3), use_container_width=True)

    st.markdown("---")

    # T038: Get dataset-specific chat history
    chat_history = get_chat_history(selected_dataset_key)

    # T049: Display conversation history
    st.subheader("ëŒ€í™” ë‚´ì—­")

    for msg in chat_history:
        with st.chat_message(msg['role']):
            st.markdown(msg['content'])

    # T047, T048: Question input and send
    user_question = st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...")

    if user_question:
        # Add user message to history
        chat_history.append({
            'role': 'user',
            'content': user_question
        })

        # Display user message
        with st.chat_message('user'):
            st.markdown(user_question)

        # T047: Generate response with streaming
        with st.chat_message('assistant'):
            try:
                # Create Anthropic client
                client = Anthropic(api_key=api_key)

                # v1.1.2: Create data context with caching
                cache_key = f"context_{selected_dataset_key}_{len(df)}"
                if cache_key not in st.session_state:
                    st.session_state[cache_key] = create_data_context(df, selected_display_name)
                data_context = st.session_state[cache_key]

                # Prepare messages for API
                api_messages = [
                    {'role': m['role'], 'content': m['content']}
                    for m in chat_history
                ]

                # T047: Stream response using st.write_stream
                response_container = st.empty()
                full_response = ""

                stream_gen = stream_chat_response_with_tools(
                    client=client,
                    model=st.session_state.chatbot['model'],
                    messages=api_messages,
                    data_context=data_context,
                    df=df
                )

                # v1.1.3: Collect tool execution info for summary after response
                tool_executions = []
                used_fallback = False

                # T046 v1.1.3: Process stream with usage tracking and tool info collection
                for chunk in stream_gen:
                    if isinstance(chunk, dict):
                        if '__usage__' in chunk:
                            # Update token usage from final message
                            usage = chunk['__usage__']
                            st.session_state.chatbot['tokens']['input'] += usage['input_tokens']
                            st.session_state.chatbot['tokens']['output'] += usage['output_tokens']
                            st.session_state.chatbot['tokens']['total'] += (
                                usage['input_tokens'] + usage['output_tokens']
                            )
                        elif '__tool_end__' in chunk:
                            # Collect tool execution info
                            tool_executions.append(chunk['__tool_end__'])
                        elif '__fallback_start__' in chunk:
                            used_fallback = True
                        # Skip other tool events (batch_start, tool_start, batch_end, fallback_end)
                    else:
                        full_response += chunk
                        response_container.markdown(full_response + "â–Œ")

                response_container.markdown(full_response)

                # v1.1.3: Show tool summary after response (fixes position bug)
                if tool_executions:
                    total_time = sum(t['elapsed'] for t in tool_executions)
                    with st.expander(f"ğŸ”§ ì‚¬ìš©ëœ ë„êµ¬ ({len(tool_executions)}ê°œ, {total_time:.2f}ì´ˆ)", expanded=False):
                        for tool in tool_executions:
                            st.write(f"âœ… `{tool['name']}` ({tool['elapsed']:.2f}ì´ˆ)")

                if used_fallback:
                    st.caption("ğŸ’¡ ë„êµ¬ ê¸°ë°˜ ë¶„ì„ì´ ì–´ë ¤ì›Œ ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

                # Add assistant message to history
                chat_history.append({
                    'role': 'assistant',
                    'content': full_response
                })

            except Exception as e:
                error_msg = handle_chat_error(e)
                st.error(error_msg)

    # T039: Clear conversation button (dataset-specific)
    if chat_history:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ì—­ ì‚­ì œ", key="clear_chat"):
            clear_chat_history(selected_dataset_key)
            st.rerun()


def main():
    """Main application entry point."""
    # Initialize session state
    init_session_state()

    # Render sidebar (T041-T044)
    render_sidebar()

    st.title("ğŸ“Š ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™”")
    st.markdown("""
    ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™” ë„êµ¬ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€
    7ê°œì˜ ê³µê³µ ë°ì´í„°ì…‹ì„ íƒìƒ‰í•˜ê³  ëŒ€í™”í˜• ì‹œê°í™”ë¥¼ í†µí•´ ê³µê°„ íŒ¨í„´ì„ ë°œê²¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.
    """)

    # Create tabs - T013: í”„ë¡œì íŠ¸ ê°œìš” first, T014: tab names, T015: ë°ì´í„° ì§ˆì˜ì‘ë‹µ ì¶”ê°€
    # v1.1.3: êµì°¨ ë°ì´í„° ë¶„ì„ íƒ­ ì‚­ì œ (10ê°œ â†’ 9ê°œ)
    tabs = st.tabs([
        "ğŸ“– í”„ë¡œì íŠ¸ ê°œìš”",
        "ğŸ¥ CCTV",
        "ğŸ’¡ ë³´ì•ˆë“±",
        "ğŸ« ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­",
        "ğŸ…¿ï¸ ì£¼ì°¨ì¥",
        "ğŸš— ì‚¬ê³ ",
        "ğŸ“Š í›ˆë ¨ ë°ì´í„°",
        "ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„°",
        "ğŸ’¬ ë°ì´í„° ì§ˆì˜ì‘ë‹µ"
    ])

    # Tab 0: Project Overview (with upload)
    with tabs[0]:
        render_overview_tab()

    # Tab 1: CCTV
    with tabs[1]:
        render_dataset_tab('cctv', 'CCTV')

    # Tab 2: Security Lights
    with tabs[2]:
        render_dataset_tab('lights', 'ë³´ì•ˆë“±')

    # Tab 3: Child Protection Zones
    with tabs[3]:
        render_dataset_tab('zones', 'ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­')

    # Tab 4: Parking Lots
    with tabs[4]:
        render_dataset_tab('parking', 'ì£¼ì°¨ì¥')

    # Tab 5: Accident
    with tabs[5]:
        render_dataset_tab('accident', 'ì‚¬ê³ ')

    # Tab 6: Train Data (T014: renamed)
    with tabs[6]:
        render_dataset_tab('train', 'í›ˆë ¨ ë°ì´í„°')

    # Tab 7: Test Data (T014: renamed)
    with tabs[7]:
        render_dataset_tab('test', 'í…ŒìŠ¤íŠ¸ ë°ì´í„°')

    # v1.1.3: êµì°¨ ë°ì´í„° ë¶„ì„ íƒ­ ì‚­ì œë¨ (render_cross_analysis_tab í•¨ìˆ˜ëŠ” ìœ ì§€)

    # Tab 8: Chatbot (T045-T050) - v1.1.3: íƒ­ ì¸ë±ìŠ¤ 9 â†’ 8
    with tabs[8]:
        render_chatbot_tab()


if __name__ == "__main__":
    main()
